import sys
import boto3
import traceback
from datetime import datetime
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.context import SparkContext
from pyspark.sql import SparkSession
from pyspark.sql.functions import lit

# Initialize GlueContext and SparkContext
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)

# Parse job parameters
args = getResolvedOptions(sys.argv, ['JOB_NAME'])
job.init(args['JOB_NAME'], args)

# Initialize boto3 clients
dynamodb = boto3.resource('dynamodb')
job_runs_table = dynamodb.Table('JobRuns')
pipeline_failures_table = dynamodb.Table('PipelineFailures')

# Log job run start
def log_job_run_start(job_name):
    try:
        response = job_runs_table.put_item(
            Item={
                'JobName': job_name,
                'RunId': job.run_id,
                'StartTime': datetime.utcnow().isoformat(),
                'Status': 'STARTED'
            }
        )
        print(f"Job run start logged: {response}")
    except Exception as e:
        print(f"Error logging job run start: {e}")

# Log job run failure
def log_job_run_failure(job_name, error_message):
    try:
        response = pipeline_failures_table.put_item(
            Item={
                'JobName': job_name,
                'RunId': job.run_id,
                'FailureTime': datetime.utcnow().isoformat(),
                'ErrorMessage': error_message
            }
        )
        print(f"Job run failure logged: {response}")
    except Exception as e:
        print(f"Error logging job run failure: {e}")

# Main script logic
try:
    log_job_run_start(args['JOB_NAME'])
    
    # Example processing logic
    # Replace this with your actual Glue job logic
    # Read data from a data source
    datasource = glueContext.create_dynamic_frame.from_catalog(database = "your_database", table_name = "your_table")

    # Process data
    processed_data = datasource.toDF().withColumn('processed', lit(True))

    # Write data to a target
    # Example: writing to S3
    glueContext.write_dynamic_frame.from_options(
        frame = DynamicFrame.fromDF(processed_data, glueContext, "processed_data"),
        connection_type = "s3",
        connection_options = {"path": "s3://your-target-bucket/"},
        format = "parquet"
    )
    
    job.commit()
except Exception as e:
    error_message = str(e)
    print(f"Job failed: {error_message}")
    log_job_run_failure(args['JOB_NAME'], error_message)
    traceback.print_exc()
    sys.exit(1)
